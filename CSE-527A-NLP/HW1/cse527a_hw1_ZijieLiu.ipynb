{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYu8aXQzonA6"
   },
   "source": [
    "# **Assignment 1 - Language Models**\n",
    "#### **Due: September 27 (Tuesday), 2022**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6heZKTDpBqd"
   },
   "source": [
    "## **Notes**\n",
    "\n",
    "### **Introduction**\n",
    "\n",
    "Welcome to CSE 527A. The goal for the first assignment is to make sure you are familiar with all the tools you need to complete the programming assignments for the course. \n",
    "\n",
    "Each assignment contains two parts: a written and coding portion. The coding portion for each homework assignment will be delivered through a Colaboratory notebook such as this one. Please use as many as code and markdown cells to run and explain all the steps you took in order to answer each question.\n",
    "\n",
    "### **Comments/Documentation**\n",
    "\n",
    "Please follow PEP 8 style guidelines (https://peps.python.org/pep-0008/) for commenting your code. Furthermore, please remember to manually save your work once in a while. If you are connected to a hosted runtime that if for whatever reason it disconnects you will have to rerun all connected code cells.\n",
    "\n",
    "### **Getting Started**\n",
    "\n",
    "In order to compile code efficiently please pay attention to if you are using a hardware accelerator or not. If you are directly calling libraries like Tensorflow, Keras, or Pytorch, it is advised to switch to a GPU.\n",
    "\n",
    "To access a GPU, go to `Edit->Notebook settings` and in the `Hardware accelerator` dropdown choose `GPU`. \n",
    "As soon as you run a code cell, you will be connected to a cloud instance with a GPU.\n",
    "Try running the code cell below to check that a GPU is connected (select the cell then either click the play button at the top left or press `Ctrl+Enter` or `Shift+Enter`).\n",
    "\n",
    "The free version of Google Colab will provide the necessary hardware for this course. Please keep in mind the RAM and Disk Space that you are allocated and that you are not given an infinite active runtime.\n",
    "\n",
    "If your local machine has a GPU that you find outperforms the cloud GPU then you can follow the necessary documentation to use a GPU with your environment.\n",
    "\n",
    "### **Lost GPU/TPU Access on Colab**\n",
    "\n",
    "If you are not allocated a GPU or cannot connect to a GPU (limits are reached for Collab), Kaggle also provides free access to GPUs and TPUs. Please transfer your work to a Kaggle runtime instance by downloading your file on Colab as a '.ipynb' file and importing the file into Kaggle.\n",
    "\n",
    "### **Submission Instructions**\n",
    "\n",
    "We will use Gradescope for assignment submission. You can upload files individually or as part of a zip file, but if using a zip file be sure you are zipping the files directly and not a folder that contains them. Please note if designated output is cleared, you will receive a 0.\n",
    "\n",
    "To download this notebook, go to `File->Download .ipynb`.  Please rename the file to match the name in our file list. \n",
    "\n",
    "When submitting your ipython notebooks, make sure everything runs correctly if the cells are executed in order starting from a fresh session.  Note that just because a cell runs in your current session doesn't mean it doesn't rely on code that you have already changed or deleted.  If the code doesn't take too long to run, we recommend re-running everything with `Runtime->Restart and run all...`.\n",
    "\n",
    "When you upload your submission to the Gradescope assignment, you should get immediate feedback that confirms your submission was processed correctly. Note that Gradesope will allow you to submit multiple times before the deadline, and we will use the latest submission for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKiZhHZxDynm"
   },
   "source": [
    "## **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GR_t_TmDD2cq"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive # one option to load datasets\n",
    "from google.colab import files\n",
    "drive.mount('/content/gdrive')\n",
    "!nvidia-smi -L # check if using GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAXOJeg2s-CE"
   },
   "source": [
    "## **Problem 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIYVOS5YUBFA"
   },
   "source": [
    "## **1.1**\n",
    "\n",
    "Write a program to compute unsmoothed unigrams, bigrams, and trigrams (you may not import nltk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "pjOb6PN5UO3h"
   },
   "outputs": [],
   "source": [
    "def unigram(word):\n",
    "    unigram1 = {}\n",
    "    for i in word:\n",
    "        if i not in unigram1:\n",
    "            unigram1[i] = 1\n",
    "        else:\n",
    "\n",
    "            unigram1[i] += 1\n",
    "    \n",
    "    return unigram1\n",
    "\n",
    "\n",
    "def bigram(word):\n",
    "    bigram1 = {}\n",
    "    for i in range(len(word)-1):\n",
    "        com = word[i] +\" \"+ word[i+1]\n",
    "        if com not in bigram1:\n",
    "            bigram1[com] = 1\n",
    "        else:\n",
    "            bigram1[com] += 1\n",
    "\n",
    "    return bigram1\n",
    "\n",
    "\n",
    "def trigram(word):\n",
    "    trigram1 = {}\n",
    "    for i in range(len(word)-2):\n",
    "        com = word[i] +\" \"+ word[i+1] + \" \" + word[i+2]\n",
    "        if com not in trigram1:\n",
    "            trigram1[com] = 1\n",
    "        else:\n",
    "            trigram1[com] += 1\n",
    "\n",
    "    return trigram1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(count_1, count_2, count_3):\n",
    "    sum = 0\n",
    "    for i in count_1:\n",
    "        sum += count_1[i]\n",
    "    pro_unigram = {}\n",
    "    for i in count_1:\n",
    "        pro_unigram[i] = count_1[i] / sum\n",
    "\n",
    "    \n",
    "    pro_bigram = {}\n",
    "    for i in count_2:\n",
    "        divid = i.split(\" \")[1]\n",
    "        pro_bigram[i] = count_2[i] / count_1[divid]\n",
    "\n",
    "    \n",
    "    pro_trigram = {} \n",
    "    keys = list(count_3.keys())\n",
    "\n",
    "    cur = []\n",
    "    for i in range(len(keys)):\n",
    "        cur.extend(keys[i].split(\" \"))\n",
    "        com_did = cur[1] + ' ' + cur[2]\n",
    "        divend = count_2[com_did]\n",
    "        par = count_3[keys[i]] / divend\n",
    "        pro_trigram[keys[i]] = par\n",
    "\n",
    "\n",
    "    \n",
    "    return pro_unigram, pro_bigram, pro_trigram\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nW1mvZAiUEAq"
   },
   "source": [
    "## **1.2**\n",
    "\n",
    "Train your model on the Wikitext-2-v1 training corpus (https://huggingface.co/datasets/wikitext). Explain the differences between your most common unigrams, bigrams, trigrams (pad beginning and end of your sentences and please remove puncutation and unknown tokens from corpus).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "gkheb8MSUPou"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (2.5.1)\n",
      "Requirement already satisfied: pandas in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from datasets) (1.4.2)\n",
      "Requirement already satisfied: multiprocess in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from datasets) (2022.2.0)\n",
      "Requirement already satisfied: responses<0.19 in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: xxhash in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from datasets) (9.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: aiohttp in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.9.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: dill<0.3.6 in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: packaging in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: filelock in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from packaging->datasets) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/liuzijie/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/Users/liuzijie/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65fe3d6d284749e5853d868d3f36c6ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 36718\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "## your code here\n",
    "!pip install datasets\n",
    "import unicodedata\n",
    "from datasets import list_datasets, load_dataset\n",
    "import string\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", 'wikitext-2-v1' )\n",
    "print(dataset)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "def process_data(wiki_data):\n",
    "    pun = get_punctuation()\n",
    "\n",
    "    pun.add('<unk>')\n",
    "\n",
    "\n",
    "    vocab = {'pad': 0, '<start>': 1, '<end>': 2}\n",
    "    data = {\"train\": [], \"validation\": [], \"test\": []}\n",
    "\n",
    "    for type in ['test', 'train', 'validation']:\n",
    "        for idx, line in enumerate(wiki_data[type]):\n",
    "            text = line['text'].strip()\n",
    "            words = [i.strip() for i in text.split() if i.strip() not in pun]\n",
    "            if len(words) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                for word in words:\n",
    "                    if word not in vocab:\n",
    "                        vocab[word] = len(vocab)\n",
    "  \n",
    "                data[type].append(\" \".join(words))\n",
    "\n",
    "    return data, vocab\n",
    "\n",
    "\n",
    "\n",
    "def get_punctuation():\n",
    "    pun = set()\n",
    "    for cp in range(17 * 65536):\n",
    "        char = chr(cp)\n",
    "        if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
    "                (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
    "            pun.add(char)\n",
    "        cat = unicodedata.category(char)\n",
    "        if cat.startswith(\"P\"):\n",
    "            pun.add(char)\n",
    "    return pun\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.3**\n",
    "\n",
    "Calculate the perplexity for each n-gram (unigram, bigram, and trigram) on all splits (traning, validataion, test sets of Wikitext-2). And, without writing code, discuss what might happen to the perplexity if you continue to increase the number of words in your n-gram (4-gram, 5-gram, etc.)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "m2XCUHbSUR2-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cal_perplexity(pro):\n",
    "    p = 0\n",
    "    values_list = list(pro.values())\n",
    "    # print(len(values_list))\n",
    "    for i in values_list:\n",
    "        p += np.log(i)\n",
    "    #     # p = math.pow(pro[keys_list[i]], -1/len(values_list))\n",
    "        a = - p / len(values_list)\n",
    "        perp = 2 ** a \n",
    "\n",
    "    return perp \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of unigram in test set is : 1985.554379948258\n",
      "The perplexity of bigram in test set is : 15.610586099937263\n",
      "The perplexity of Trigram in test set is : 0.9494600761856709\n",
      "                                                               \n",
      "The perplexity of unigram in validation set is : 1883.698481889108\n",
      "The perplexity of bigram in validation set is : 14.681681512382676\n",
      "The perplexity of Trigram in validation set is : 4.70062424354332\n",
      "                                                               \n",
      "The perplexity of unigram in train set is : 2003.057839147159\n",
      "The perplexity of bigram in train set is : 98.2681028472694\n",
      "The perplexity of Trigram in train set is : 6.4820862937523\n"
     ]
    }
   ],
   "source": [
    "data, vocab = process_data(dataset)\n",
    "\n",
    "for i in [\"test\", \"validation\", \"train\"]:\n",
    "    sign_data = ['start']\n",
    "    for j in data[i]:\n",
    "        sign_data.extend(j.split(\" \"))\n",
    "    sign_data.append('end')\n",
    "    \n",
    "    count_1 = unigram(sign_data)\n",
    "    count_2 = bigram(sign_data)\n",
    "    count_3 = trigram(sign_data)\n",
    "    \n",
    "    pro_unigram, pro_bigram, pro_trigram = calculate(count_1, count_2, count_3)\n",
    "    \n",
    "    \n",
    "    result1 = cal_perplexity(pro_unigram)\n",
    "    result2 = cal_perplexity(pro_bigram)\n",
    "    result3 = cal_perplexity(pro_trigram)\n",
    "    print(\"The perplexity of unigram in \" + i + \" set is :\", result1) #2003.057839147159\n",
    "    print(\"The perplexity of bigram in \" + i + \" set is :\", result2)\n",
    "    print(\"The perplexity of Trigram in \" + i + \" set is :\", result3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7GvYRzjUK9x"
   },
   "source": [
    "## **1.4**\n",
    "\n",
    "Enable Laplace smoothing and Add-K smoothing (0.1,0.05,0.01) to your code. Discuss the changes in perplexity values between n-grams as you try different smoothing methods/values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "aVkncRe1USvc"
   },
   "outputs": [],
   "source": [
    "## your code here\n",
    "\n",
    "\n",
    "def calculate_laplace(count_1, count_2, count_3):\n",
    "    sum = 0\n",
    "    for i in count_1:\n",
    "        sum += count_1[i]\n",
    "    pro_unigram = {}\n",
    "    for i in count_1:\n",
    "        pro_unigram[i] = (count_1[i] + 1) / (sum + len(vocab))\n",
    "\n",
    "    \n",
    "    pro_bigram = {}\n",
    "    for i in count_2:\n",
    "        divid = i.split(\" \")[1]\n",
    "        pro_bigram[i] = (count_2[i] + 1) / (count_1[divid] + len(vocab))\n",
    "\n",
    "    \n",
    "    pro_trigram = {} \n",
    "    keys = list(count_3.keys())\n",
    "    cur = []\n",
    "    for i in range(len(keys)):\n",
    "        cur.extend(keys[i].split(\" \"))\n",
    "        com_did = cur[1] + ' ' + cur[2]\n",
    "        divend = count_2[com_did]\n",
    "        par = count_3[keys[i]] + 1 / divend + len(vocab)\n",
    "        pro_trigram[keys[i]] = par\n",
    "    \n",
    "    return pro_unigram, pro_bigram, pro_trigram\n",
    "\n",
    "\n",
    "  \n",
    "def calculate_addK(K, count_1, count_2, count_3):\n",
    "    sum = 0\n",
    "    for i in count_1:\n",
    "        sum += count_1[i]\n",
    "\n",
    "    pro_unigram = {}\n",
    "    for i in count_1:\n",
    "        pro_unigram[i] = (count_1[i] + K) / (sum + K * len(vocab))\n",
    "\n",
    "    \n",
    "    pro_bigram = {}\n",
    "    for i in count_2:\n",
    "        divid = i.split(\" \")[1]\n",
    "        pro_bigram[i] = (count_2[i] + K) / (count_1[divid] + K * len(vocab))\n",
    "\n",
    "    \n",
    "    pro_trigram = {} \n",
    "    keys = list(count_3.keys())\n",
    "    cur = []\n",
    "    for i in range(len(keys)):\n",
    "        cur.extend(keys[i].split(\" \"))\n",
    "        com_did = cur[1] + ' ' + cur[2]\n",
    "        divend = count_2[com_did]\n",
    "        par = (count_3[keys[i]] + K) / (divend + K * len(vocab))\n",
    "        pro_trigram[keys[i]] = par\n",
    "    \n",
    "    return pro_unigram, pro_bigram, pro_trigram\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of unigram in testset with laplace smoothing is: 4020.891842558921\n",
      "The perplexity of bigram in testset with laplace smoothing is: 100.03398193161631\n",
      "The perplexity of Trigram in testset with laplace smoothing is: 55.375952726011484\n",
      "K= 0.1\n",
      "The perplexity of unigram in testset with add-K smoothing is : 1944.4564747625152\n",
      "The perplexity of Bigram in testset with add-K smoothing is : 248.16163719375461\n",
      "The perplexity of Trigram in testset with add-K smoothing is : 246.1126345233863\n",
      "K= 0.05\n",
      "The perplexity of unigram in testset with add-K smoothing is : 1964.347637783388\n",
      "The perplexity of Bigram in testset with add-K smoothing is : 169.49316229450773\n",
      "The perplexity of Trigram in testset with add-K smoothing is : 157.01515024815285\n",
      "K= 0.01\n",
      "The perplexity of unigram in testset with add-K smoothing is : 1981.2009225578959\n",
      "The perplexity of Bigram in testset with add-K smoothing is : 73.94672662303982\n",
      "The perplexity of Trigram in testset with add-K smoothing is : 52.88515163727598\n",
      "The perplexity of unigram in validationset with laplace smoothing is: 1981.2009225578959\n",
      "The perplexity of bigram in validationset with laplace smoothing is: 73.94672662303982\n",
      "The perplexity of Trigram in validationset with laplace smoothing is: 52.88515163727598\n",
      "K= 0.1\n",
      "The perplexity of unigram in validationset with add-K smoothing is : 1846.1258375654163\n",
      "The perplexity of Bigram in validationset with add-K smoothing is : 247.82483170495803\n",
      "The perplexity of Trigram in validationset with add-K smoothing is : 247.39900282971317\n",
      "K= 0.05\n",
      "The perplexity of unigram in validationset with add-K smoothing is : 1864.2932124493605\n",
      "The perplexity of Bigram in validationset with add-K smoothing is : 168.7230826591615\n",
      "The perplexity of Trigram in validationset with add-K smoothing is : 158.14529943055197\n",
      "K= 0.01\n",
      "The perplexity of unigram in validationset with add-K smoothing is : 1879.7118537732406\n",
      "The perplexity of Bigram in validationset with add-K smoothing is : 72.8569401429599\n",
      "The perplexity of Trigram in validationset with add-K smoothing is : 54.06013235840762\n"
     ]
    }
   ],
   "source": [
    "data, vocab = process_data(dataset)\n",
    "\n",
    "for i in [\"test\", \"validation\"]:\n",
    "    sign_data = ['start']\n",
    "    for j in data[i]:\n",
    "        sign_data.extend(j.split(\" \"))\n",
    "    sign_data.append('end')\n",
    "    \n",
    "    count_1 = unigram(sign_data)\n",
    "    count_2 = bigram(sign_data)\n",
    "    count_3 = trigram(sign_data)\n",
    "    \n",
    "    pro_unigram_la, pro_bigram_la, pro_trigram_la = calculate_laplace(count_1, count_2, count_3) \n",
    "    \n",
    "    result1 = cal_perplexity(pro_unigram)\n",
    "    result2 = cal_perplexity(pro_bigram)\n",
    "    result3 = cal_perplexity(pro_trigram)\n",
    "    print(\"The perplexity of unigram in \" + i + \"set with laplace smoothing is:\", result1) \n",
    "    print(\"The perplexity of bigram in \" + i + \"set with laplace smoothing is:\", result2)\n",
    "    print(\"The perplexity of Trigram in \" + i + \"set with laplace smoothing is:\", result3)\n",
    "    \n",
    "    for j in [0.1, 0.05, 0.01]:\n",
    "        pro_unigram, pro_bigram, pro_trigram = calculate_addK( j, count_1, count_2, count_3)\n",
    "        result1 = cal_perplexity(pro_unigram)\n",
    "        result2 = cal_perplexity(pro_bigram)\n",
    "        result3 = cal_perplexity(pro_trigram)\n",
    "        print(\"K=\",j)\n",
    "        print(\"The perplexity of unigram in \" + i + \"set with add-K smoothing is :\", result1) #2003.057839147159\n",
    "        print(\"The perplexity of Bigram in \" + i + \"set with add-K smoothing is :\", result2)\n",
    "        print(\"The perplexity of Trigram in \" + i + \"set with add-K smoothing is :\", result3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of unigram in trainset with laplace smoothing is: 1981.2009225578959\n",
      "The perplexity of bigram in trainset with laplace smoothing is: 73.94672662303982\n",
      "The perplexity of Trigram in trainset with laplace smoothing is: 52.88515163727598\n",
      "K= 0.1\n",
      "The perplexity of unigram in trainset with add-K smoothing is : 3988.3851522610685\n",
      "The perplexity of Bigram in trainset with add-K smoothing is : 281.8580179853999\n",
      "The perplexity of Trigram in trainset with add-K smoothing is : 242.465570050648\n",
      "K= 0.05\n",
      "The perplexity of unigram in trainset with add-K smoothing is : 4006.3164333053182\n",
      "The perplexity of Bigram in trainset with add-K smoothing is : 201.68171795263464\n",
      "The perplexity of Trigram in trainset with add-K smoothing is : 155.7188918604515\n",
      "K= 0.01\n",
      "The perplexity of unigram in trainset with add-K smoothing is : 4020.891842558921\n",
      "The perplexity of Bigram in trainset with add-K smoothing is : 100.03398193161631\n",
      "The perplexity of Trigram in trainset with add-K smoothing is : 55.375952726011484\n"
     ]
    }
   ],
   "source": [
    "data, vocab = process_data(dataset)\n",
    "\n",
    "for i in [\"train\"]:\n",
    "    sign_data = ['start']\n",
    "    for j in data[i]:\n",
    "        sign_data.extend(j.split(\" \"))\n",
    "    sign_data.append('end')\n",
    "    \n",
    "    count_1 = unigram(sign_data)\n",
    "    count_2 = bigram(sign_data)\n",
    "    count_3 = trigram(sign_data)\n",
    "    \n",
    "    pro_unigram_la, pro_bigram_la, pro_trigram_la = calculate_laplace(count_1, count_2, count_3) \n",
    "    \n",
    "    result1 = cal_perplexity(pro_unigram)\n",
    "    result2 = cal_perplexity(pro_bigram)\n",
    "    result3 = cal_perplexity(pro_trigram)\n",
    "    print(\"The perplexity of unigram in \" + i + \"set with laplace smoothing is:\", result1) \n",
    "    print(\"The perplexity of bigram in \" + i + \"set with laplace smoothing is:\", result2)\n",
    "    print(\"The perplexity of Trigram in \" + i + \"set with laplace smoothing is:\", result3)\n",
    "    \n",
    "    for j in [0.1, 0.05, 0.01]:\n",
    "        pro_unigram, pro_bigram, pro_trigram = calculate_addK( j, count_1, count_2, count_3)\n",
    "        result1 = cal_perplexity(pro_unigram)\n",
    "        result2 = cal_perplexity(pro_bigram)\n",
    "        result3 = cal_perplexity(pro_trigram)\n",
    "        print(\"K=\",j)\n",
    "        print(\"The perplexity of unigram in \" + i + \"set with add-K smoothing is :\", result1) #2003.057839147159\n",
    "        print(\"The perplexity of Bigram in \" + i + \"set with add-K smoothing is :\", result2)\n",
    "        print(\"The perplexity of Trigram in \" + i + \"set with add-K smoothing is :\", result3)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-ce94w5tAnP"
   },
   "source": [
    "## **Problem 2**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNWxGLhaByZX"
   },
   "source": [
    "(Eisenstein Ch. 6) Using the Pytorch library, train an LSTM language model from the same Wikitext training corpus you used in problem 1. After each epoch of training, compute its perplexity on the Wikitext validation corpus. Stop training when the perplexity stops improving.\n",
    "\n",
    "1. Fully describe your model architecture, hyperparameters, and experimental procedure.\n",
    "2. After each epoch of training, compute your LM’s perplexity on the development data. Plot the development perplexity against # of epochs. Additionally, compute and report the perplexity on test\n",
    "data.\n",
    "3. Compare experimental results such as perplexity and training time between your n-gram and neural models (include smoothed and unsmooth n-grams). Provide graphs that demonstrate your\n",
    "results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gAYjEiJtvJt"
   },
   "outputs": [],
   "source": [
    "## your code here\n",
    "\n",
    "## your code here\n",
    "# coding: utf-8\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from io import open\n",
    "import torch\n",
    "import json\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.emsize = 200\n",
    "        self.nhid = 200\n",
    "        self.nlayers = 3\n",
    "        self.lr = 20.0\n",
    "        self.clip = 0.25\n",
    "\n",
    "        self.epochs = 4\n",
    "        self.batch_size = 20\n",
    "        # seq length\n",
    "        self.bptt = 35\n",
    "        self.dropout = 0.2\n",
    "        self.tied = False\n",
    "        self.seed = 123\n",
    "        self.log_interval = 500\n",
    "        self.save = 'model.pt'\n",
    "\n",
    "args = Config()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# get punctuationc\n",
    "def get_punctuation():\n",
    "    pun = set()\n",
    "   \n",
    "    for cp in range(17 * 65536):\n",
    "        char = chr(cp)\n",
    "        if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
    "                (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
    "            pun.add(char)\n",
    "        cat = unicodedata.category(char)\n",
    "        if cat.startswith(\"P\"):\n",
    "            pun.add(char)\n",
    "    return pun\n",
    "\n",
    "\n",
    "puns = get_punctuation()\n",
    "\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {\"<unk>\":0}\n",
    "        self.idx2word = [\"<unk>\"]\n",
    "        \n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self):\n",
    "        self.dictionary = Dictionary()\n",
    "        data = load_dataset(\"wikitext\", \"wikitext-2-v1\")\n",
    "        print(f\"raw train sample: {data['train']}\")\n",
    "        print(f\"raw train sample: {data['validation']}\")\n",
    "        print(f\"raw train sample: {data['test']}\")\n",
    "        # validation\n",
    "        self.train = self.tokenize(data['train'])\n",
    "        self.valid = self.tokenize(data['validation'])\n",
    "        self.test = self.tokenize(data['test'])\n",
    "        print(f\"vocab size: {len(self.dictionary.word2idx)}\")\n",
    "        \n",
    "\n",
    "    def tokenize(self, data):\n",
    "      \n",
    "        for line in data:\n",
    "            line = line['text'].strip()\n",
    "            words = line.split() + ['<eos>']\n",
    "            for word in words:\n",
    "                if word in puns:\n",
    "                    continue\n",
    "                self.dictionary.add_word(word)\n",
    "\n",
    "        idss = []\n",
    "        for line in data:\n",
    "            line = line['text'].strip()\n",
    "            words = line.split() + ['<eos>']\n",
    "            ids = []\n",
    "            for word in words:\n",
    "                if word in puns:\n",
    "                    continue\n",
    "                ids.append(self.dictionary.word2idx[word])\n",
    "            idss.append(torch.tensor(ids).type(torch.int64))\n",
    "        ids = torch.cat(idss)\n",
    "\n",
    "        return ids\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Model architecture\n",
    "###############################################################################\n",
    "\n",
    "    \n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.ntoken = ntoken\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        if tie_weights:\n",
    "            if nhid != ninp:\n",
    "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.bias)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output)\n",
    "        decoded = decoded.view(-1, self.ntoken)\n",
    "        return F.log_softmax(decoded, dim=1), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Load data\n",
    "###############################################################################\n",
    "\n",
    "corpus = Corpus()\n",
    "\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(corpus.train, args.batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)\n",
    "\n",
    "###############################################################################\n",
    "# Build the model\n",
    "###############################################################################\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "\n",
    "model = RNNModel(ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.tied).to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "###############################################################################\n",
    "# Training code\n",
    "###############################################################################\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(args.bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target\n",
    "\n",
    "\n",
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, args.bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output, hidden = model(data, hidden)\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            total_loss += len(data) * criterion(output, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(args.batch_size)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, args.bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        model.zero_grad()\n",
    "        \n",
    "        hidden = repackage_hidden(hidden)\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(p.grad, alpha=-lr)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % args.log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / args.log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // args.bptt, lr,\n",
    "                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        \n",
    "\n",
    "\n",
    "# Loop over epochs.\n",
    "lr = args.lr\n",
    "best_val_loss = None\n",
    "\n",
    "records = []\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        record = {\"epoch\":epoch}\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        record['epoch_cost_time'] = time.time() - epoch_start_time\n",
    "        val_loss = evaluate(val_data)\n",
    "        \n",
    "\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "        record['valid_loss'] = val_loss\n",
    "        record['vaild_ppl'] = math.exp(val_loss)\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(args.save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "        # Run on test data.\n",
    "        test_loss = evaluate(test_data)\n",
    "        print('=' * 89)\n",
    "        print('| end of epoch {:3d} | test loss {:5.2f} | test ppl {:8.2f}'.format(epoch, test_loss, math.exp(test_loss)))\n",
    "        record['test_loss'] = test_loss\n",
    "        record['test_ppl'] = math.exp(test_loss)\n",
    "        print('=' * 89)\n",
    "        print(record)\n",
    "        records.append(record)\n",
    "        with open(\"records.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join([json.dumps(i, ensure_ascii=False) for i in records]))\n",
    "        \n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')\n",
    "\n",
    "\n",
    "    \n",
    "###############################################################################\n",
    "# Print info\n",
    "###############################################################################\n",
    "\n",
    "for record in records:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "name_list = ['Monday','Tuesday','Friday','Sunday']\n",
    "num_list = [1985.554379948258,0.6,7.8,6]\n",
    "num_list1 = [1,2,3,1]\n",
    "x =list(range(len(num_list)))\n",
    "total_width, n = 0.8, 2\n",
    "width = total_width / n\n",
    " \n",
    "plt.bar(x, num_list, width=width, label='boy',fc = 'y')\n",
    "for i in range(len(x)):\n",
    "    x[i] = x[i] + width\n",
    "plt.bar(x, num_list1, width=width, label='girl',tick_label = name_list,fc = 'r')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
